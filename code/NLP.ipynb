{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'userID' from 'credentials' (/opt/anaconda3/lib/python3.8/site-packages/credentials/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6450327af818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPIkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \"\"\"\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'userID' from 'credentials' (/opt/anaconda3/lib/python3.8/site-packages/credentials/__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from credentials import userID, APIkey\n",
    "\n",
    "\"\"\"\n",
    "Reads documents with split sentences, classifies them into pro-/con-/non-arguments, and writes out arguments and their stance information.\n",
    "\"\"\"\n",
    "\n",
    "def read_doc(path):\n",
    "    with open(path, \"r\") as in_f:\n",
    "        return json.load(in_f)\n",
    "\n",
    "def get_doc_list(path):\n",
    "    if os.path.isfile(path+\"../processed_docs.txt\"):\n",
    "        with open(path+\"../processed_docs.txt\", \"r\") as in_f:\n",
    "            processed_docs = in_f.readlines()\n",
    "            if len(processed_docs) == 0:\n",
    "                return sorted(os.listdir(path))\n",
    "            processed_docs = processed_docs[0].split(\",\")\n",
    "            if processed_docs[-1] == \"\":\n",
    "                processed_docs = processed_docs[:-1]\n",
    "            processed_docs = set(processed_docs)\n",
    "            all_docs = set(sorted(os.listdir(path)))\n",
    "            return sorted(all_docs.difference(processed_docs)) #a different b, return different items\n",
    "    else:\n",
    "        return sorted(os.listdir(path))\n",
    "#what does this step do?\n",
    "def get_out_number(doc_path):\n",
    "    \"\"\"\n",
    "    If new documents are gathered after previous documents were already processed via aspect detection, we start a new final.jsonl file\n",
    "    and store the ID so the aspect_classification.py process knows where to start from.\n",
    "    \"\"\"\n",
    "    latest_file_ids = sorted([int(f.replace(\"final-\", \"\").replace(\".jsonl\", \"\")) for f in os.listdir(doc_path) if f.startswith(\"final-\")])\n",
    "    if len(latest_file_ids) == 0:\n",
    "        return 0\n",
    "    new_id = latest_file_ids[-1] + 1\n",
    "    with open(doc_path+\"../aspect_detection_starting_doc_id.txt\", \"w\") as out_f:\n",
    "        out_f.write(str(new_id))\n",
    "    return new_id\n",
    "\n",
    "\n",
    "def write_data(topic, doc_id_start, topic_word_list, MAX_FILE_SIZE, FILTER_TOPIC, out_path):\n",
    "    print('Start classifying sentences for topic \"{0}\" from doc_id_start {1} '\n",
    "          'with MAX_FILE_SIZE {2}, and FILTER_TOPIC set \"{3}\"\". Writing to {4}'.format(topic, str(doc_id_start),\n",
    "                                                                                   str(MAX_FILE_SIZE),\n",
    "                                                                                   str(FILTER_TOPIC), str(out_path)))\n",
    "\n",
    "    try:\n",
    "        os.makedirs(out_path + \"/processed/\")\n",
    "    except FileExistsError:\n",
    "        # directory already exists\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        doc_path = out_path + \"/unprocessed/\"\n",
    "        out_path = out_path + \"/processed/\"\n",
    "        docs = get_doc_list(doc_path)\n",
    "        first_line = True\n",
    "        total_id = 0\n",
    "        total_processed = 0\n",
    "        out_number = get_out_number(out_path) # each time this method is called, it starts a new document\n",
    "        current_file_size = 0\n",
    "        sents_skipped_relevance = 0\n",
    "        total_duplicates = 0\n",
    "        for doc_id in tqdm(range(doc_id_start, len(docs))):\n",
    "            doc = docs[doc_id]\n",
    "            if current_file_size >= MAX_FILE_SIZE:\n",
    "                out_number += 1\n",
    "                first_line = True\n",
    "                current_file_size = 0\n",
    "\n",
    "            with open(doc_path+doc, \"r\") as in_f, open(out_path+\"final-{0}.jsonl\".format(out_number), \"a\") as out_f,\\\n",
    "                open(doc_path+\"../processed_docs.txt\", \"a\") as out_processed_docs_f:\n",
    "                data = json.load(in_f)\n",
    "\n",
    "                sent_ids = list(range(0,len(data['sents'])))\n",
    "                if FILTER_TOPIC == True:\n",
    "                    for sent_i, sent in enumerate(data['sents']):\n",
    "                        total_processed += 1\n",
    "                        if not True in [True for tok in topic_word_list if tok.lower() in sent.lower()]:\n",
    "                            sent_ids.remove(sent_i)\n",
    "                            sents_skipped_relevance += 1\n",
    "\n",
    "\n",
    "                sents_to_send = [data['sents'][s] for s in sent_ids]\n",
    "                if len(sents_to_send) == 0:\n",
    "                    continue\n",
    "\n",
    "                payload = {\n",
    "                        \"topic\": topic,\n",
    "                        \"showOnlyArguments\": False,\n",
    "                        \"computeAttention\": False,\n",
    "                        \"sortBy\": \"none\",\n",
    "                        \"predictStance\": True,\n",
    "                        \"sentences\": sents_to_send,\n",
    "                        \"userID\": userID,\n",
    "                        \"apiKey\": APIkey\n",
    "                    }\n",
    "\n",
    "                is_timed_out = True\n",
    "                timed_out_ctr = 1\n",
    "                while is_timed_out == True:\n",
    "                    try:\n",
    "                        json_dict = requests.post(\"https://api.argumentsearch.com/en/classify\", timeout=300, data=json.dumps(payload),\n",
    "                                                          headers={'Content-Type': 'application/json'}).json()\n",
    "                        is_timed_out = False\n",
    "                    except requests.exceptions.Timeout or ConnectionError:\n",
    "\n",
    "                        print(\"Timed out for {0} times.\".format(str(timed_out_ctr)))\n",
    "\n",
    "\n",
    "                header = { #keys from the document json-file that will be taken over to the training data samples\n",
    "                    \"doc_id\": data.get(\"id\", -1),\n",
    "                    \"doc_metadata_id\": data.get(\"metadata_id\", \"N/A\"),\n",
    "                    \"doc_url\": data.get(\"url\", \"N/A\"),\n",
    "                    \"doc_score\": data.get(\"score\", -1),\n",
    "                    \"index\": data.get(\"index\", \"N/A\")\n",
    "                }\n",
    "\n",
    "\n",
    "                duplicates = set()\n",
    "                for doc_sent_id, sent in zip(sent_ids, json_dict['sentences']):\n",
    "                    if sent['sentenceOriginal'] in duplicates:\n",
    "                        total_duplicates += 1\n",
    "                        continue\n",
    "\n",
    "                    duplicates.add(sent['sentenceOriginal'])\n",
    "\n",
    "                    if sent[\"argumentLabel\"] == \"argument\":\n",
    "                        row = {\"id\": doc_id_start+total_id,\n",
    "                               \"doc_sent_id\": doc_sent_id,\n",
    "                               \"stance\": \"Argument_for\" if sent[\"stanceLabel\"] == \"pro\" else \"Argument_against\",\n",
    "                               \"sent\": sent[\"sentenceOriginal\"]}\n",
    "                        row.update(header)\n",
    "\n",
    "                        if first_line == False:\n",
    "                            out_f.write(\"\\n\")\n",
    "                        json.dump(row, out_f)\n",
    "                        first_line = False\n",
    "                        total_id += 1\n",
    "                        current_file_size += 1\n",
    "                        out_processed_docs_f.write(doc+\",\")\n",
    "\n",
    "        print_out = str(datetime.now()) + \" - Total sentences skipped due to topic-relevance: {0}\\n\".format(str(sents_skipped_relevance))\n",
    "        print_out += str(datetime.now()) + \" - Total duplicates skipped: {0}\\n\".format(str(total_duplicates))\n",
    "        print_out += str(datetime.now()) + \" - Total sentences left: {0}\\n\".format(str(total_id))\n",
    "        print_out += str(datetime.now()) + \" - Total sentences processed: {0}\\n\".format(str(total_processed))\n",
    "        print_out += str(datetime.now()) + \" - The rest of the sentence were filtered out because they are no arguments.\"\n",
    "        print(print_out)\n",
    "\n",
    "        with open(out_path+\"../note.txt\", \"a\") as note_out_f:\n",
    "            note_out_f.write(\"\\n\\n\"+str(datetime.now()) + \" - arg_classification.py:\\n\")\n",
    "            note_out_f.write(print_out)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Crashed at doc_id {0}\".format(str(doc_id)))\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Classifies arguments from raw docs')\n",
    "parser.add_argument('--topic', type=str, required=True,\n",
    "                                        help='Topic without underscore.')\n",
    "parser.add_argument('--doc_id_start', type=int, default=0,\n",
    "                                        help='The doc id from which to start after crash. At the beginning defaults to 0.')\n",
    "parser.add_argument('--max_file_size', type=int, default=200000,\n",
    "                                        help='Maximum lines of a document before splitting it.')\n",
    "parser.add_argument('--filter_topic', type=int, default=1,\n",
    "                                        help='Filters out sentences that do not contain a token from the topic or its defined synonyms.')\n",
    "parser.add_argument('--index', type=str, default=\"common-crawl-en\", required=True,\n",
    "                                        help='Data source index name.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "training_data_path = \"../../training_data/\"\n",
    "\n",
    "topic = args.topic\n",
    "doc_id_start = args.doc_id_start\n",
    "MAX_FILE_SIZE = args.max_file_size\n",
    "out_path = \"{0}{1}/{2}/\".format(training_data_path, args.index, topic.replace(\" \", \"_\"))\n",
    "FILTER_TOPIC = True if args.filter_topic == 1 else False\n",
    "\n",
    "topic_word_dict = { # topic synonyms to pre-filter sentences prior to argument and stance classification\n",
    "    \"school uniforms\": [\"uniform\", \"college\", \"outfit\", \"dress\", \"suit\", \"jacket\", \"cloth\"],\n",
    "    \"nuclear energy\": [\"fission\", \"fusion\", \"atomic energy\", \"nuclear power\", \"atomic power\", \"radioactive\", \"radioactivity\"],\n",
    "    \"marijuana legalization\": [\"cannabis\", \"legalization of marijuana\", \"legal\", \"illegal\", \"law\", \"weed\", \"dope\"],\n",
    "    \"cloning\": [\"clone\", \"cloned\", \"duplicate\", \"copy\", \"reproduct\", \"asexual\"],\n",
    "    \"death penalty\": [\"capital punishment\", \"execution\", \"electric chair\", \"punishment\", \"punish\"],\n",
    "    \"minimum wage\": [\"living wage\", \"base pay\", \"average wage\", \"low income\"],\n",
    "    \"abortion\": [\"abort\", \"termination\", \"misbirth\", \"birth control\"],\n",
    "    \"gun control\": [\"second amendment\", \"ownership\", \"arms reduction\", \"arms limitation\"],\n",
    "}\n",
    "\n",
    "if topic not in topic_word_dict.keys():\n",
    "    topic_word_dict[topic] = []\n",
    "topic_word_dict[topic].extend(topic.split(\" \"))\n",
    "write_data(topic, doc_id_start, topic_word_dict[topic], MAX_FILE_SIZE, FILTER_TOPIC, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'userID' from 'credentials' (/opt/anaconda3/lib/python3.8/site-packages/credentials/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-49f505057278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPIkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'userID' from 'credentials' (/opt/anaconda3/lib/python3.8/site-packages/credentials/__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from credentials import userID, APIkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'userID' from 'credentials' (/opt/anaconda3/lib/python3.8/site-packages/credentials/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a6efb53558eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPIkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'userID' from 'credentials' (/opt/anaconda3/lib/python3.8/site-packages/credentials/__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/wanglian'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
